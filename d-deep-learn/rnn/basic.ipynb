{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f2d06e6",
   "metadata": {},
   "source": [
    "RNN and LSTM \n",
    "\n",
    "resources:\n",
    "\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning#nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "703e47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e614539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11343/3246371683.py:2: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
      "  df = yf.download(ticker, start='2024-01-01', end='2025-07-01')\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>248.419998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>238.449997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>237.929993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05</th>\n",
       "      <td>237.490005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08</th>\n",
       "      <td>240.449997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-24</th>\n",
       "      <td>340.470001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-25</th>\n",
       "      <td>327.549988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-26</th>\n",
       "      <td>325.779999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-27</th>\n",
       "      <td>323.630005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-06-30</th>\n",
       "      <td>317.660004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Price            Close\n",
       "Ticker            TSLA\n",
       "Date                  \n",
       "2024-01-02  248.419998\n",
       "2024-01-03  238.449997\n",
       "2024-01-04  237.929993\n",
       "2024-01-05  237.490005\n",
       "2024-01-08  240.449997\n",
       "...                ...\n",
       "2025-06-24  340.470001\n",
       "2025-06-25  327.549988\n",
       "2025-06-26  325.779999\n",
       "2025-06-27  323.630005\n",
       "2025-06-30  317.660004\n",
       "\n",
       "[374 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker='TSLA'\n",
    "df = yf.download(ticker, start='2024-01-01', end='2025-07-01')\n",
    "df = df[['Close']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(df)\n",
    "sequence_length = 30\n",
    "\n",
    "def create_sequence(data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i+sequence_length])\n",
    "        y.append(data[i+sequence_length])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18f71d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 32, 12])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#input_size es el numero de features del input\n",
    "#hidden_state es el numero de features en el hidden state\n",
    "#que coincide con el hidde de salida\n",
    "rnn = nn.LSTM(input_size=3, hidden_size=12, num_layers=3, batch_first=True)\n",
    "\n",
    "#el input tiene forma NxLxHin\n",
    "#Hin is el input size\n",
    "x = torch.randn(5, 32, 3)\n",
    "x.shape\n",
    "output, _  = rnn(x)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3668203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float)\n",
    "        self.y = torch.tensor(y, dtype=torch.float)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd84a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_sequence(scaled, sequence_length)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.2, shuffle=False)\n",
    "train_dataset = StockDataset(x_train, y_train)\n",
    "test_dataset = StockDataset(x_test, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47bd800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    # input es de la forma NxLxHin\n",
    "    # N es tamaño de batch \n",
    "    # L es tamaño de la secuencia\n",
    "    # Hin es tamaño del input\n",
    "    def forward(self, x):\n",
    "        # x es Nx10x1\n",
    "        # out es Nx10x64\n",
    "        out, hidden = self.lstm(x)\n",
    "        # out es Nx64\n",
    "        out = out[:, -1, :]\n",
    "        # out es Nx1\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e29195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in epoch 10, loss 0.00238\n",
      "loss in epoch 20, loss 0.00194\n",
      "loss in epoch 30, loss 0.00166\n",
      "loss in epoch 40, loss 0.00141\n",
      "loss in epoch 50, loss 0.00134\n",
      "loss in epoch 60, loss 0.00122\n",
      "loss in epoch 70, loss 0.00123\n",
      "loss in epoch 80, loss 0.00119\n",
      "loss in epoch 90, loss 0.00116\n",
      "loss in epoch 100, loss 0.00125\n",
      "loss in epoch 110, loss 0.00110\n",
      "loss in epoch 120, loss 0.00112\n",
      "loss in epoch 130, loss 0.00119\n",
      "loss in epoch 140, loss 0.00113\n",
      "loss in epoch 150, loss 0.00116\n",
      "loss in epoch 160, loss 0.00120\n",
      "loss in epoch 170, loss 0.00120\n",
      "loss in epoch 180, loss 0.00114\n",
      "loss in epoch 190, loss 0.00105\n",
      "loss in epoch 200, loss 0.00108\n",
      "loss in epoch 210, loss 0.00112\n",
      "loss in epoch 220, loss 0.00107\n",
      "loss in epoch 230, loss 0.00104\n",
      "loss in epoch 240, loss 0.00103\n",
      "loss in epoch 250, loss 0.00103\n",
      "loss in epoch 260, loss 0.00099\n",
      "loss in epoch 270, loss 0.00104\n",
      "loss in epoch 280, loss 0.00102\n",
      "loss in epoch 290, loss 0.00098\n",
      "loss in epoch 300, loss 0.00102\n",
      "loss in epoch 310, loss 0.00096\n",
      "loss in epoch 320, loss 0.00103\n",
      "loss in epoch 330, loss 0.00089\n",
      "loss in epoch 340, loss 0.00093\n",
      "loss in epoch 350, loss 0.00087\n",
      "loss in epoch 360, loss 0.00082\n",
      "loss in epoch 370, loss 0.00087\n",
      "loss in epoch 380, loss 0.00082\n",
      "loss in epoch 390, loss 0.00074\n",
      "loss in epoch 400, loss 0.00085\n",
      "loss in epoch 410, loss 0.00073\n",
      "loss in epoch 420, loss 0.00070\n",
      "loss in epoch 430, loss 0.00072\n",
      "loss in epoch 440, loss 0.00071\n",
      "loss in epoch 450, loss 0.00062\n",
      "loss in epoch 460, loss 0.00071\n",
      "loss in epoch 470, loss 0.00057\n",
      "loss in epoch 480, loss 0.00067\n",
      "loss in epoch 490, loss 0.00060\n",
      "loss in epoch 500, loss 0.00057\n",
      "loss in epoch 510, loss 0.00054\n",
      "loss in epoch 520, loss 0.00047\n",
      "loss in epoch 530, loss 0.00053\n",
      "loss in epoch 540, loss 0.00053\n",
      "loss in epoch 550, loss 0.00053\n",
      "loss in epoch 560, loss 0.00052\n",
      "loss in epoch 570, loss 0.00041\n",
      "loss in epoch 580, loss 0.00042\n",
      "loss in epoch 590, loss 0.00053\n",
      "loss in epoch 600, loss 0.00053\n",
      "loss in epoch 610, loss 0.00041\n",
      "loss in epoch 620, loss 0.00035\n",
      "loss in epoch 630, loss 0.00032\n",
      "loss in epoch 640, loss 0.00028\n",
      "loss in epoch 650, loss 0.00031\n",
      "loss in epoch 660, loss 0.00032\n",
      "loss in epoch 670, loss 0.00043\n",
      "loss in epoch 680, loss 0.00033\n",
      "loss in epoch 690, loss 0.00040\n",
      "loss in epoch 700, loss 0.00030\n",
      "loss in epoch 710, loss 0.00024\n",
      "loss in epoch 720, loss 0.00029\n",
      "loss in epoch 730, loss 0.00021\n",
      "loss in epoch 740, loss 0.00024\n",
      "loss in epoch 750, loss 0.00019\n",
      "loss in epoch 760, loss 0.00021\n",
      "loss in epoch 770, loss 0.00028\n",
      "loss in epoch 780, loss 0.00019\n",
      "loss in epoch 790, loss 0.00017\n",
      "loss in epoch 800, loss 0.00019\n",
      "loss in epoch 810, loss 0.00013\n",
      "loss in epoch 820, loss 0.00020\n",
      "loss in epoch 830, loss 0.00018\n",
      "loss in epoch 840, loss 0.00014\n",
      "loss in epoch 850, loss 0.00014\n",
      "loss in epoch 860, loss 0.00011\n",
      "loss in epoch 870, loss 0.00012\n",
      "loss in epoch 880, loss 0.00013\n",
      "loss in epoch 890, loss 0.00011\n",
      "loss in epoch 900, loss 0.00011\n",
      "loss in epoch 910, loss 0.00009\n",
      "loss in epoch 920, loss 0.00015\n",
      "loss in epoch 930, loss 0.00006\n",
      "loss in epoch 940, loss 0.00025\n",
      "loss in epoch 950, loss 0.00010\n",
      "loss in epoch 960, loss 0.00012\n",
      "loss in epoch 970, loss 0.00014\n",
      "loss in epoch 980, loss 0.00006\n",
      "loss in epoch 990, loss 0.00007\n",
      "loss in epoch 1000, loss 0.00008\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_dataloader:\n",
    "        out = model(x_batch)\n",
    "        loss = criterion(out, y_batch)\n",
    "        #print(f' x is {x_batch} , shape is {x_batch.shape}')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"loss in epoch {epoch + 1}, loss {total_loss/len(train_dataloader):.5f}\")    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed0a959c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d = df.iloc[-sequence_length:]\n",
    "d = scaler.fit_transform(d)\n",
    "x_test = torch.tensor(d, dtype=torch.float)\n",
    "x_out = torch.unsqueeze(x_test, 0)\n",
    "x_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4924975a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 32, 3])\n",
      "predition value is [[332.1742]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "print(x.shape)\n",
    "with torch.no_grad():\n",
    "    prediction = model(x_out).numpy()\n",
    "    pred_original = scaler.inverse_transform(prediction)\n",
    "    print(f\"predition value is {pred_original}\")\n",
    "    \n",
    "#scaler.inverse_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6aef0dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameter after reset:\n",
      "lstm.weight_ih_l0 : -0.1103\n",
      "lstm.weight_hh_l0 : -0.0069\n",
      "lstm.bias_ih_l0 : -0.0705\n",
      "lstm.bias_hh_l0 : -0.0769\n",
      "fc.weight : 0.0039\n",
      "fc.bias : 0.1302\n"
     ]
    }
   ],
   "source": [
    "def reset_weights(model):\n",
    "    for layer in model.children():\n",
    "        if hasattr(layer, 'reset_paramters'):\n",
    "            layer.reset_parameters()\n",
    "\n",
    "reset_weights(model)\n",
    "\n",
    "print(\"model parameter after reset:\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name} : {param.data.mean():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsroad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
